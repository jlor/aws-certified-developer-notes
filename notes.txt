- S3
    - Details
        - Max 100 buckets
        - Unique bucket names, DNS name convention
        - Multiple regions. Data stays in region unless explicitly moved.
        - Min size: 0b
        - Max size: 5Tb
        - > 5Gb requires multipart upload
        - Multipart upload recommended on >= 100Mb
        - Multipart can upload parallel and out of order
        - CompleteMultipartUpload API reassembles the file after multipart upload
        - Can be encrypted before WRITE to disk, decrypted ON download
    - Data Consistency
        - New objects: Read-after-write consistency
            - No stale reads possible
            - Potential higher read latency
            - Potential lower read throughput
        - Updated objects: Eventual consistency
            - Stale reads possible
            - Lowest read latency
            - Highest read throughput
    - Performance
        - Consistent:
            - > 100 put/list/update requests/s
            - > 300 get requests/s
        - Burst:
            - > 300 put/list/update requests/s
            - > 800 get requests/s
        - Request more if needed prior to production
    - Performance Optimizations
            - S3 keeps index of object keynames in each region
            - BAD:
                - ex/2016-12-07/p1.jpg
                - ex/2016-12-07/p2.jpg
                - ex/2016-12-07/p3.jpg
                - ex/2016-12-07/p4.jpg
            - GOOD:
                - ex/e87f-2016-12-07/p1.jpg
                - ex/a023-2016-12-07/p2.jpg
                - ex/b753-2016-12-07/p3.jpg
                - ex/18fa-2016-12-07/p4.jpg
            - Requests don't all float through same node (indexes built on hashes rather than the same date string)
            - GET heavy workflow => use CloudFront CDN as caching service
                - Limits the requests on S3
    - Hosting static websites
        - HTML/CSS/Javascript in Buckets.
            - Custom error pages
            - Custom index file
            - Custom redirect rules
        - S3 gives default URL
            - <bucket-name>.s3-website-<AWS region>.amazonaws.com
            - Route 53 integration for custom names
        - Bucket name must match domain name (dashsoft.dk -> S3://dashsoft.dk, www.dashsoft.dk -> s3://www.dashsoft.dk)
            - if someone used that bucket name, feature can't be used
    - S3 IAM and Bucket policies
        - more: https://aws.amazon.com/blogs/security/iam-policies-and-bucket-policies-and-acls-oh-my-controlling-access-to-s3-resources/
        - IAM policy: User level (User policy)
            - Multiple users can be assigned same IAM policy
            - Attached to a user, so can not be used to grant anonymous users permissions
            - JSON based 
        - Bucket policy: Resource level (resource-based policy)
            - JSON file attached to resource
                - Who is allowed to access resources?
                - What that user can do with those resources?
                - Can contain conditions (e.g. StringEquals)
            - max 20kb size
            - SHOULD BE USED to manage cross-account permissions for all Amazon S3 permissions
        - ACLs:
            - Legacy.
            - For buckets and objects
            - More restrictive than Bucket Policies
                - Can manage permissions on INDIVIDUAL OBJECTS WITHIN A BUCKET
            - XML based
            - Grant read/write to OTHER AWS ACCOUNTS
            - No conditional permissions
            - Can not explicitly deny permissions
            - Only way to manage access to objects not owned by the bucket owner
        - Bucket owner has full permission as default
            - Bucket owner paying bills can deny access or modify objects regardless of who owns them
        - Explicit DENY always overrides allows in policies.
    - Logging S3 API calls
        - CloudTrail
            - Bucket levels operations
            - Logs all API calls
            - Use with CloudWatch to filter certain calls and notify on occurences
        - Amazon S3 Server Access Logs
            - Object level operations
            - Logs GET
            - May take up to 1 hour before showing logs
            - on a best effort basis - not for completely accurate logging!!
        - Saved to S3 bucket
    - Object Versioning
        - Three states: Versioning enabled, Versioning suspended, unversioned
            - Once version-enabled a bucket cannot be unversioned.
            - Version-enabled can be version suspended
                - Existing versions will remain.
                - New objects will be given version ID null
        - Setup at bucket level
        - New versions of objects are given a Version ID
        - Upon GET the latest version is returned
        - Upon DELETE all versions remain in bucket, but a delete marker will be inserted on the object. Will appear to be deleted.
        - Permanent deletion of version: Specify ID. Next ID will be current version.
        - Restore a version:
            - Copy from ID (old versions ID) to same bucket
                - works because: GET -> PUT (gets new ID)
            - Delete current versions ID (i.e. next ID will be current)
        - Lifecycle management to handle versions life. Examples:
            - Send noncurrent versions to amazon Glacier?
            - Permanently delete objects that have been non-current for 180 days
    - S3 Encryption
        - Protecting data in-transit
            - SSL or client-side encryption or...
            - AWS KMS-managed (Key Management System) Customer Master Key (CMK)
                - Unique encryption key for each object
                - Amazon knows master key
                - On upload:
                    - Client request key from AWS KMS
                    - KMS returns key: plain text + cipher blob
                    - plain text used to encrypt object data
                    - cipher blob to upload to S3 as object metadata
                - On download:
                    - Client downloads encrypted object data + cipher blob in object metadata
                    - Client sends cipher blob to KMS and gets plain text back
                    - plain text used to decrypt object data
            - Client-side master key:
                - Amazon does NOT know master key
                - on upload:
                    - Client provides master key to AWS S3 encryption client (LOCALLY)
                    - S3 client generates random data key and encrypts it with master key
                    - S3 client encrypts data using data key and uploads material description as part of object metadata (x-amz-meta-x-amz-key)
                - on download:
                    - Client downloads encrypted object WITH metadata
                    - Metadata tells client which master key to use
                    - Client decrypts the data key using that master key
                    - Client uses decrypted data key to decrypt object
        - Protecting data at rest
            - Server side encryption (by AWS S3)
                - Must add x-amz-server-side-encryption request header to upload request
                - Uses AES-256 
                - Bucket policy can require all objects to use server-side encryption
                    - StringNotEquals "x-amz-server-side-encryption" = "AES-256"
            - KMS-managed encryption keys
                - Uses customer master keys
                - More flexibility in controlling keys
            - Customer-provided encryption keys
                - Gives the option to generate your own keys outside the environment
                - Amazon does NOT store the encryption key
        - Both can be used simultaneously
- DynamoDB
    - Components
        - Table: Collection of data (items). Think DB table.
        - Item: Collection of attributes. Think tuples.
        - Attribute: Fundamental data element. Think DB fields/columns.
        - Schemaless: Each item can have its own distinct attributes - other than primary key!
        - Scalar attributes: Most attributes are scalar meaning they can only have one value. (Strings, numbers & Binary)
        - Nested attributes: Nested attributes can support up to 32 levels nesting depth
        - Primary Key
            - Partition key: A single attribute that distinguish the item from all others. Also known as the "hash attribute".
            - Composite key (partition key + sort key): Partition key is fed to hash function to determine physical storage location. Multiple items with same partition key will be placed on same partition sorted according to the sort key! Sort key also known as the "range attribute".
            - Must be of scalar value: String, Number, Binary.
        - Secondary indexes: Allows quering using alternate key (in addition to queries on primary key)
            - Global (GSI): Index with partition key + sort key that can be different from those on the table (max 5 per table)
                - Consume THEIR OWN provision throughput for read/write (specified upon index creation)
                - Can be created AFTER table creation
                - Updated asynchronuously => changes are EVENTUALLY CONSISTENT
            - Local (LSI): Index that has same partition key as the table, but different sort key. (max 5 per table)
                - Consume SAME read/write throughput from original table
                    - Every partition is scoped to a table partition with the same partition key value
                - Can only be created DURING table creation
                - Can be queried using either eventual consistent or strongly consistent.
            - Index will belong to a table (the base table). 
            - Can hold 5 LSI and 5 GSI for a total of 10 indexes per table.
            - Indexes are maintained automatically by DynamoDB.
            - Upon Index creation you can specify which attributes should be "projected" or copied from base table to index. DynamoDB will as a minimum project key attributes (partition key + sort key)
                - KEYS_ONLY (default. Smallest index, more performant)
                - INCLUDE (custom attributes included)
                - ALL  (INCLUDE *.. Biggest index, least performance)
            - Querying an attribute not in primary key/secondary indexes is still possible, but will be queried on main table - NOT index. Will cost more latency+read capacity
        - Read/Write capacity
            - Used for partitioning across multiple servers
        - Streams
            - Optional
            - Captures data modification events represented by a "stream record" on:
                - New item added: Stream captures an image of the entire item.
                - Item updated: Stream captures before and after image of modified attributes.
                - Item deleted: Stream captures image of entire item before deletion.
            - Contains:
                - Name of table
                - Event timestamp
                - other metadata
            - A record lives for 24 hours and is then removed from the stream
            - Can be used as a trigger for AWS Lambda.
                - Example: New Customer entries trigger Lambda which in turn trigger Simple Email Service (SES) to send welcome mail to said customer.
    - Overview & limits
        - Fully managed NoSQL
        - Built in Monitoring
        - Consistent and performance
            - Data stored on SSDs
            - Can control throughput read/write capacity
            - Can spread load between servers/tables
            - Replication across multiple availability zones in an AWS region
        - Different levels of consistency
            - Eventually consistent
            - Strongly consistent
        - Conditional updates & concurrency control
            - Conditional writes
            - Atomic counter
        - Integration with Elastic MapReduce & RedShift
        - Streams
        - Provisioned Throughput
            - Flexible read/write performance capacity
                - Set during table creation
                - Can be changed at anytime without downtime or performance degradation
            - Ability to reserve capacity
        - Limits
            - 256 tables per region (increase upon request)
            - partition key length: 2048 byte max (2kb), 1 byte min
            - Sort key length: 1024 byte max (1kb), 1 byte min
            - Item size: 400kb including attribute name + value
            - API specific limits
                - Table updates (CreateTable, UpdateTable, DeleteTable): 10 simultaneous actions
                - BatchGetItem returns max 100 items
                    - Must be less than 16mb
                - BatchWriteItem can contain 25 PutItem or DeleteItem
                    - Must be less than 16mb
                - Query & Scan result set is limited to 1mb of data per call
                    - LastEvaluatedKey included in response to continue retrieval from last returned element
    - Provisioned Throughput
        - Unit of read capacity
            - For items up to 4kb:
                - 1 strongly consistent read per second
                - 2 eventually consistent read per second
        - Unit of write capacity
            - For items up to 1kb
                - 1 write per second
        - Calculating read capacity
            - Round up to nearest 4kb multiplier
                - Items that are 3kb in size still use 4kb data. Can do 1 strongly consistent read or 2 eventually consistent reads per second
            - Example:
                - item size 3kb
                - Want to read 80 items per second from the table
                - how many read capacity units required?
                    - Take 1 item, round UP to nearest 4kb multiplier. 
                    - Take that number divide by 4 => this is how many reads you need for ONE item
                    - Multiply with items to read per second. Divide by 1 for STRONG consistency, 2 for eventual consistency:
                        - 80 * (3kb [rounded to 4] / 4kb) / 1 read/s = 80 units of STRONGLY CONSISTENT read capacity
                        - 80 * (3kb [rounded to 4] / 4kb) / 2 read/s = 40 units of EVENTUALLY CONSISTENT read capacity
            - Local secondary indexes
                - Uses read/write capacity from parent table
                - If only reading index keys and projected attributes the calculations are equal to regular reads
                    - Calculate using the size of index entry, NOT table item size
                    - Rounded up to nearest 4kb
                    - Cost saving because => index entry size < table item size!
                - If querying non-projected attributes / non-keys we get extra latency and read capacity cost:
                    - Uses read capacity from index and EVERY item from the table - not just the attribute needed
                    - Attribute projection VERY important
            - Global secondary indexes
                - Have their own throughput capacity
                - Only support eventually consistent reads => 8kb/read capacity unit (4 kb * 2 reads/s)
                - Calculate using the size of index entry, NOT table item entry
                - Cost saving because => index entry size < table item size!
        - Calculating write capacity
            - Round up to nearest 1kb multiplier
            - Example:
                - item size 1.5kb
                - 10 writes per second
                - how many write capacity units required?
                    - Take 1 item, round UP to nearest 1kb multiplier
                    - Take that number, divide by 1 => this is how many writes you need for ONE item
                    - Multiply with items to read per second.
                        - 10 * (1.5kb [rounded to 2] / 1kb / 1 write/s) = 20 units of write capacity
            - Local secondary indexes
                - Uses read/write capacity from parent table
                - ADDing items, UPDATEing items and DELETEing items also uses table write capacity
                    - Adding item with projected attribute costs 1 write operation to put into index
                        - Add
                    - Updating item with previously undefined attribute that is projected to index: 1 write operation
                        - Add
                    - Updating item with previously DEFINED attribute that is projected to index: 2 writes
                        - Delete
                        - Add
                    - Delete item with attribute projected to index: 1 write
                        - Delete
            - Global secondary indexes
                - Have their own throughput capacity
                - Follow same capacity rules as local secondary indexes
        - Exceeding throughput
            - Requests exceeding throughput may be throttled
            - For global secondary indexes, ALL indexes must have enough write capacity or the WRITE may be throttled -- even if the write doesn't affect the index
                - This includes any local secondary indexes as well as keys
    - Query vs. Scan API calls
        - Query
            - Finds items using PRIMARY KEY values from table or secondary index
            - Defaults to return all matching items
            - Will only return result up to 1mb. Will include lastEvaluatedKey entry to continue retrieval from last returned element 
                - If LastEvaluatedKey == null => reached last matching item
            - Defaults to Eventually consistent, can be set to strongly consistent
                - Global secondary indexes (GSI) only support EVENTUALLY consistent
            - Can use conditional operators and filters to return precise results
            - Returns all attributes of an item, or only the ones you want
            - Data returned in sorted order by sort key value (ascending default)
        - Scan
            - Reads every item in a table
            - Can only return ResultSets up to 1mb in size
            - Benefits
                - Can apply filters to the results to refine values
                - Can return only specific attributes with the "ProjectionExpression" parameter
                - ResultSet includes LastEvaluatedKey entry to continue retrieval from last returned element
            - Negatives
                - Filters applied after data has been scanned
                    - The more filters the slower performance. 
                    - The larger the data the slower performance.
                - Returns only filtered results
                - Only eventual consistent reads available
            - Keep in mind
                - Can reduce Page Size with the "Limit" parameter, to limit how much data you try to retrieve at the same time
                    - Reduce chance of being throttled
                - Avoid performing scans on mission-critical tables
                    - Throughput allocated per table! => Scans may impact available read/writes for the application!!
                    - Possible to create tables with duplicated contents. Application can then rotate which table to pull data from and perform scans on "non-active" tables
                - If throttled (response code: You exceeded your provisioned throughput) try again with exponential backoff
                - Can program logic to increase throughput (using Update Table operation)
                    - Once requests die down a bit => back off on provision throughput
    - Conditional Writes & Atomic Counters
        - Atomic Counters
            - Increment/decrement value of an attribute without interfering with other write requests
            - Requests are applied in order they are received
            - Updates are NOT idempotent - it will update teh value each time it is called
            - use case: Increasing view count
        - Conditional Writes
            - Checks for a condition before proceeding with the operation
            - Supported for PutItem, DeleteItem, UpdateItem operations
            - Specify conditions in ConditionExpression
                - Can contain attribute names, conditional operators, and built-in functions
            - For this reason updates ARE idempotent!!
            - Failed conditional writes return ConditionalCheckFailedException
    - Connecting to DynamoDB from your app using Identity Providers
        - Can use temporary security credentials to make calls to AWS services
        - Example: 
            - Users on mobile app need ability to communicate with DynamoDB.
            - No hardcoded credentials in the app => would need to push new client in case of updated credentials
        - Options:
            - Web Identity Federation
                - Authentication token provided from:
                    - Web identity federation: Facebook, Google, etc. (OpenID Connect 2.0 -- OIDC) providers
                    - Enterprise identity federation: Authentication of users in organizations network
                - Authentication token exchanged for temporary credentials in AWS which in turn map to an IAM role with permission to use DynamoDB.
            - Amazon Cognito
                - Preferred way to manage identities
                    - Handles identity federation work for us
                - Cognito Identity
                    - Create unique identities for users
                    - Authenticate identities with identity providers - or our own auth process
                    - Supports unauthenticated identities (anonymous)
                    - Save mobile user data
                    - Use credentials obtained to sync data with Cognito Sync
                - Cognito Sync
                    - Sync user data across mobile devices and the web
                    - Client libraries cache data locally
            - IAM roles
                - We need roles for our users to assume
                - Role gives access to DynamoDB
                - Temporary credentials are associated with a specific IAM role.

